{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d6c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622aeda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'y', 'X'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat(\"face_emotion_data.mat\")\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21056f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 9)\n",
      "(128, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de286b24",
   "metadata": {},
   "source": [
    "# a)\n",
    "Use the training data X and y and a least squares problem to train your classifier weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc61cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classier Weights :\n",
      " [[ 0.94366942]\n",
      " [ 0.21373778]\n",
      " [ 0.26641775]\n",
      " [-0.39221373]\n",
      " [-0.00538552]\n",
      " [-0.01764687]\n",
      " [-0.16632809]\n",
      " [-0.0822838 ]\n",
      " [-0.16644364]]\n"
     ]
    }
   ],
   "source": [
    "w = np.linalg.inv(X.T@X)@X.T@y\n",
    "print(\"Classier Weights :\\n\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5db289",
   "metadata": {},
   "source": [
    "# b)\n",
    "Explain how to use the weights you found to classify a new face image as happy or angry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b68f9",
   "metadata": {},
   "source": [
    "9-by-1 feature vector, v\n",
    "<br>\n",
    "y = singn(v.T@w)\n",
    "<br>\n",
    "y = 1 -> happy\n",
    "<br>\n",
    "y = -1 -> angry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97693589",
   "metadata": {},
   "source": [
    "# c)\n",
    "Which features seem to be most important? Justify your answer. Note that the nine columns of the training data feature matrix X have been normalized to have the same two-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae236435",
   "metadata": {},
   "source": [
    "Features with the largest absolute values of w\n",
    "<br>\n",
    "X: x1,x2,...,x9\n",
    "<br>\n",
    "y = sign(x1w1+x2w2+...+x9w9)\n",
    "<br>\n",
    "9 cols of training data feature matrix X --normalized--> same 2-norm\n",
    "<br>\n",
    "X_norm = data/np.linalg,norm(data,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875d5a2",
   "metadata": {},
   "source": [
    "# d)\n",
    "Design a classifier based on three of the nine features. Which three should you choose? Describe the procedure for designing your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0179d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Classifier Weight:\n",
      " [[ 0.70546316]\n",
      " [ 0.8737872 ]\n",
      " [-0.78805643]]\n"
     ]
    }
   ],
   "source": [
    "X_s = X[:,[0,2,3]]\n",
    "w_s = np.linalg.inv(X_s.T@X_s)@X_s.T@y\n",
    "print(\"New Classifier Weight:\\n\", w_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff18f2",
   "metadata": {},
   "source": [
    "# e)\n",
    "What percent of the training labels are incorrectly classified using all nine features? What percent of the training labels are incorrectly classified using your reduced set of three features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e3e411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error percentage using 9 features:  2.34%\n",
      "Error percentage using 3 feature:  6.25%\n"
     ]
    }
   ],
   "source": [
    "y1 = np.sign(X@w)\n",
    "e1 = y - y1\n",
    "num_1 = 0\n",
    "for e in e1:\n",
    "    if e!=0:\n",
    "        num_1 += 1\n",
    "print(\"Error percentage using 9 features: \", \"%.2f%%\"%(num_1/len(e1)*100))\n",
    "\n",
    "y2 = np.sign(X_s@w_s)\n",
    "e2 = y - y2\n",
    "num_2 = 0\n",
    "for e in e2:\n",
    "    if e!=0:\n",
    "        num_2 += 1\n",
    "print(\"Error percentage using 3 feature: \", \"%.2f%%\"%(num_2/len(e2)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec0f02",
   "metadata": {},
   "source": [
    "# f)\n",
    "Now use cross validation to assess your classifier performance. Divide the available data in to eight subsets of sixteen samples (e.g., examples 1−16, 17−32, . . . , 113− 128). Use seven sets to design your classifier weights, then use the remaining hold-out set to evaluate the classifier performance. Compute the number of mis- classifications made on this hold-out set and divide that number by 16 (the size of the set) to estimate the error rate for that hold-out set. Repeat this process eight times using the eight different possible divisions between training and hold-out sets and average the error rates to obtain a final performance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe61822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error percentage:  4.69%\n"
     ]
    }
   ],
   "source": [
    "e_list = []\n",
    "for i in range(8):\n",
    "    x_eval = X[i*16:i*16+16,:]\n",
    "    y_eval = y[i*16:i*16+16,:]\n",
    "    x_train = np.vstack((X[:i*16,:],X[i*16+16:,:]))\n",
    "    y_train = np.vstack((y[:i*16,:],y[i*16+16:,:]))\n",
    "    w_train = np.linalg.inv(x_train.T@x_train)@x_train.T@y_train\n",
    "    y3 = np.sign(x_eval@w_train)\n",
    "    e3 = np.mean((y_eval-y3)/2)\n",
    "    e_list.append(e3)\n",
    "e_mean = np.mean(e_list)\n",
    "print(\"average error percentage: \", \"%.2f%%\"%(e_mean*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
